// https://math.stackexchange.com/questions/3421665/do-the-vertical-bar-and-semicolon-denote-the-same-thing-in-the-context-of-condit#:~:text=1%20Answer&text=Although%20some%20writers%20are%20a,similar%2C%20but%20not%20quite%20identical.

Q: 

  In the CMU Machine Learning Lecture, likelihood function is denoted by ğ‘ƒ(ğ·|ğœƒ)

  In the Cornell lecture note, likelihood function is denoted by ğ‘ƒ(ğ·;ğœƒ)

A:

- The `Sheffer stroke` (vertical bar) is read **"given that"** or **"contingent upon."** Thus ğ‘(ğ‘¥|ğ‘¦) means __the probability density of ğ‘¥ given that some event or state of affairs ğ‘¦ has occurred__. 
For instance if ğ‘¥ represents a height in centimeters, the term ğ‘(ğ‘¥|woman) means the probability density we find the height value ğ‘¥ given that the person we are measuring is a woman.
This is classical terminology of `contingent probability density`. This use is restricted to probability and statistics.

- The semicolon is slightly different, and can apply in cases __unrelated to probability and statistics__. Suppose you have a mathematical function that depends upon two (or more) variables, 
say `ğ‘“(ğ‘¥,ğ‘¦)`. Suppose you're primarily interested in the behavior of ğ‘“ based on the value of ğ‘¥. You want to take derivatives with respect to ğ‘¥, 
limits for large and small ğ‘¥, count the zeros, and such. When you write __ğ‘“(ğ‘¥;ğ‘¦) you're in essence creating a new function OF ONE VARIABLE (ğ‘¥)__, 
which we might call `ğ‘”(ğ‘¥)=ğ‘“(ğ‘¥;ğ‘¦)`. This notation recognizes that there is an implied or chosen value of ğ‘¦ when you examine ğ‘”(ğ‘¥), but it is not __"part of the function."__
For instance, it is meaningless to __take the derivative of ğ‘“(ğ‘¥;ğ‘¦) with respect to ğ‘¦ any more than it would to take the derivative of ğ‘”(ğ‘¥) with respect to ğ‘¦. 
The value of ğ‘¦ is a "setting" or a "parameter" that is fixed.__

A particularly useful application of this semicolon notation in probability is in the `Expectation-Maximization (or EM)` Algorithm. 
In the algorithm some steps involve the optimization of a function (the expectation) with respect to one of the many variables. 
It is natural, then, to use `ğ‘“(ğ‘¥;ğ‘¦)` during this step to optimize over ğ‘¥, where the value of ğ‘¦ is fixed and __"cannot be touched."__

Another use in statistics and pattern recognition is the following. Suppose you have training data, `ğ‘¥1,ğ‘¥2,â€¦,ğ‘¥ğ‘‘.` You create the dot product with some real-valued 
weight vector `ğ°={ğ‘¤1,ğ‘¤2,â€¦,ğ‘¤ğ‘‘}`, as in `ğ°ğ‘¡ğ±=ğ‘¤1ğ‘¥1+ğ‘¤2ğ‘¥2+â€¦+ğ‘¤ğ‘‘ğ‘¥ğ‘‘`. You want to optimize the value of `ğ°` with respect to some classification. 
You create a function `â„(ğ°;ğ±)`, meaning you can take derivatives with respect to the weights but it makes NO SENSE to take a derivative with respect to the data ğ±, 
nor is `ğ° "contingent upon" ğ±`. Nevertheless, the particular functional form of â„ has buried behind it the particular values of the data at hand.

