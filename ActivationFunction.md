
[Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)

- sigmoid function

  - it is nonlinear in nature. Combinations of this function are also nonlinear
  - the output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function
  - vanishing gradients

- ReLu

  - ReLu is nonlinear in nature. And combinations of ReLu are also non linear
  - activations sparse and efficient
  - dying ReLu problem - leaky ReLu (y = 0.01x for x<0)
